1. Ma, Y.J., Liang, W., Wang, G., Huang, D.A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L. and Anandkumar, A., 2023. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931.

2. Cunningham, H., Ewart, A., Riggs, L., Huben, R. and Sharkey, L., 2023. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600.

3. Rai, D., Zhou, Y., Feng, S., Saparov, A. and Yao, Z., 2024. A practical review of mechanistic interpretability for transformer-based language models. arXiv preprint arXiv:2407.02646.

4. Lin, Z., Basu, S., Beigi, M., Manjunatha, V., Rossi, R.A., Wang, Z., Zhou, Y., Balasubramanian, S., Zarei, A., Rezaei, K. and Shen, Y., 2025. A survey on mechanistic interpretability for multi-modal foundation models. arXiv preprint arXiv:2502.17516.

5. Tas, O.S. and Wagner, R., 2024. Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers. arXiv preprint arXiv:2406.11624.

6. Du, Y., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel, P., Gupta, A. and Andreas, J., 2023, July. Guiding pretraining in reinforcement learning with large language models. In International Conference on Machine Learning (pp. 8657-8677). PMLR.

7. Zeng, Y., Mu, Y. and Shao, L., 2024. Learning reward for robot skills using large language models via self-alignment. arXiv preprint arXiv:2405.07162.

8. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C. and Chen, C., 2022. Constitutional ai: Harmlessness from ai feedback, 2022. arXiv preprint arXiv:2212.08073, 8(3).

9. Wang S, Zhang S, Zhang J, Hu R, Li X, Zhang T, Li J, Wu F, Wang G, Hovy E. Reinforcement learning enhanced llms: A survey. arXiv preprint arXiv:2412.10400. 2024 Dec 5.