## Reward functions and LLM reflection generated by Eureka
[Reward functions](./chapters/eureka_rewards.md)

## Differences between Linear and Activation correlations with features
Referencing [this](../results/neuron_labels.txt)

ELU squashes the negative part to asymptote at -alpha (-1)

For correlations with features, the linear layers tend to have higher correlations

For phase selectivity, the activation layers tend to have higher selectivity as we rescale the smallest activation to 0 -- in linear layers, the negative part can be highly negative, so it is hard to be twice that number

When we do absolute activations (instead of offset) for selectivity, we see mainly approaching neurons while the opening neurons don't show up as much

## Other analyses

We performed a bunch of analyses (see the various notebooks) but a number of them were not used in the final report as they did not provide any additional insights.

We list some of them that appear promising and could work if further refined:

- [Base action analysis](../notebooks/eda/base_action_analysis.ipynb) plots a bunch of action dimentions to attempt to visualize the trajectories
- [PCA analysis](../notebooks/mech_interp/pca_analysis.ipynb) plots the relations between components and we do see some nonlinearities going on
